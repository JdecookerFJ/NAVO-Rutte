# -*- coding: utf-8 -*-
"""Sentiment Analyse en Named Entity Recognition - Engelse speeches Rutte bij NATO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CUUiignMLyVufUs0wR5dTy6fueSuci3u

Mark Rutte is sinds 1 oktober 2024 de secretaris-generaal van de NAVO. Hij heeft sindsdien tientallen publieke speeches, persco's of andere openbare optredens gedaan. De toespraak die het meeste stof deed opwaaien, was op 12 december 2024. In zijn eerste grote speech als NAVO-chef, voor denktank Carnegie Europe in Brussel, gaf Rutte geen geruststellend verhaal. Volgens 'onze' oud-premier is het "tijd om ons geestelijk voor te bereiden op oorlog". Er volgde veel publiciteit voor de speech. Vooral de boodschap van Rutte werd grondig geanalyseerd. De speech zou de "*gewone Nederlander* wakker hebben geschud", stelde Ren√© van Rijckevorsel in [EW Magazine](https://www.ewmagazine.nl/buitenland/opinie/2025/01/speech-rutte-heeft-gewone-nederlanders-wakker-geschud-nu-politici-en-bedrijven-nog-1443808/). Rutte had de 'noodklok' gevonden, analyseerde Michel Kerres in [NRC](https://https://www.nrc.nl/nieuws/2024/12/12/navo-chef-rutte-ziet-oorlogsgevaar-in-volle-vaart-op-ons-afkomen-a4876534).

Was de speech van Rutte een anomalie? Of is zijn toon als NAVO-baas een sombere over de hele linie? Kortom, welke toon slaat Rutte aan in zijn beginfase als secretaris-generaal?

Alle openbare speeches, interacties met de pers en persconferenties zijn openbaar en te vinden in de [pers-omgeving](https://https://www.nato.int/cps/en/natohq/opinions.htm?query=Mark%20Rutte&search_types=Opinion&display_mode=opinion&date_from=01.10.2024&date_to=dd.mm.yyyy&keywordquery=*&chunk=2) van de NAVO-website. Ik heb de eerste 65 openbare uitingen van Rutte verzameld (van 1 oktober 2024 t/m 10 maart 2025)en verzameld in een openbaar dataset

Deze teksten zijn Engelstalig en laat er nu net een lexicon met woordscores bestaan: het NRC Emotion Lexicon. Hierin zijn woordscores toegekend aan woorden: "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing." Een goede basis voor een analyse van Rutte's speeches die we hieronder in concept gaan doen.

Ook duiken we in de wondere wereld van Named Entity Recognition, waarin we uit de speeches locaties, persoonsnamen en organisatienamen kunnen extraheren. Wie o wie noemt Rutte het meest? Netanyahu? Poetin? Of toch Zelensky?

# Installeren Google Drive, klaarzetten Emolex voor sentiment analyse en koppelen Rutte dataset

We hebben voor dit projectje 2 databronnen:


1.   De NRC Lexicon
2.   Een database van alle openbare uitingen van Mark Rutte als NAVO-baas, zoals vermeld op de website van de NAVO.

We willen inzichten verzamelen over de (vermeende) toon van deze uitingen. Hoe erg zitten ze op het 'angst'-spectrum? Of hoe positief zijn ze?

Eerst gaan we de datasets inladen. Ik gebruik hiervoor een Google Drive-omgeving, maar je kan deze analyse ook lokaal werkend krijgen.

Ik neem de manier om het NRC Lexicon in te laden over van [Jonathan Soma](https://https://www.jonathansoma.com/), die trouwens een geweldige website heeft met voorbeelden van de inzet van Machine Learning en Supervised Learning op de website [Investigate.AI](https://investigate.ai/).
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline

filepath = "/content/drive/MyDrive/ProjectSentimentAnalyse/NRC emotion/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt"
emolex_df = pd.read_csv(filepath,  names=["word", "emotion", "association"], skiprows=45, sep='\t', keep_default_na=False)
emolex_df.head(12)

"""# Flexen met NRC Emolex - welke emoties, welke woorden vallen eronder? Hoe ziet de lexicon eruit?

Aan de hand van het NRC Lexicon kan je emoties toekennen aan teksten. Het is een vorm van supervised learning, waarin vooraf door mensen een score van 0 of 1 is toegekend aan een bepaald woord. 'War' scoort bijvoorbeeld een 0 in de categorie 'Vertrouwen' en een 1 voor de categorie 'Angst'. Hiermee maak je taal 'leesbaar' voor een AI-systeem. Helaas zijn er niet voor elke emotie 14150 woorden gekoppeld! 1 betekent ‚Äúis gekoppeld‚Äù en 0 betekent ‚Äúis niet gekoppeld‚Äù.

Er zitten natuurlijk heel veel menselijke keuzes en overeenstemming achter. Je moet hierin vertrouwen op de afwegingen van de makers van het NRC Lexicon...

We verkennen hieronder verder de mogelijkheden van het Lexicon. We tonen:


1.   De emoties in het Lexicon
2.   Hoeveel zitten de emoties in het Lexicon?
3. Welke woorden vallen er onder de categorie 'Verwachting'?
4. Welke woorden vallen onder de categorie 'Angst'?



 https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/upshot-trump-emolex/notebooks/NRC%20Emotional%20Lexicon.ipynb#scrollTo=NNMEDlo8lOZf

***Werk deze verder uit***
"""

emolex_df.emotion.unique()

emolex_df.emotion.value_counts()

emolex_df[emolex_df.association == 1].emotion.value_counts()

emolex_df[(emolex_df.association == 1) & (emolex_df.emotion == 'anticipation')].word

emolex_words = emolex_df.pivot(index='word', columns='emotion', values='association').reset_index()
emolex_words.head()

# Angry words
emolex_words[emolex_words.anger == 1].word

"""Je kan ook individuele woorden invoeren en kijken hoe het gelabeld is. Neem het woord 'War'/'Oorlog'."""

emolex_words[emolex_words.word == 'war']

"""# Inladen speeches Rutte

Hieronder laden we de speeches van Rutte in.
"""

#dataframe van xls-dataset Rutte Speeches
file_path = '/content/drive/MyDrive/ProjectSentimentAnalyse/NATO RUTTE DATASET/RutteNATO2025.xlsx'
df = pd.read_excel(file_path, sheet_name=0, usecols=['tekst'], dtype={'tekst': str})

df

"""# Tokeniseren, vectoriseren en matrix maken van woorden in NRC en de speeches

We willen de speeches langs het Lexicon leggen. We willen hier niet alleen de woorden tellen, maar op zoek gaan naar percentages. Hiervoor gebruiken we een TfidfVectorizer die relaties kan leggen tussen de woorden in de betreffende speeches. Als ik zeg ‚ÄúIk houd van deze tarte tatin‚Äù, zouden we ongeveer 50% van de woorden als positief analyseren (met name 'joy').
"""

from sklearn.feature_extraction.text import TfidfVectorizer

#  Load the dataset (Rutte's speeches)
file_path = '/content/drive/MyDrive/ProjectSentimentAnalyse/NATO RUTTE DATASET/RutteNATO2025.xlsx'
df = pd.read_excel(file_path, sheet_name=0, usecols=['tekst'], dtype={'tekst': str})

# Load EmoLex
filepath = "/content/drive/MyDrive/ProjectSentimentAnalyse/NRC emotion/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt"
emolex_df = pd.read_csv(filepath, names=["word", "emotion", "association"], skiprows=45, sep='\t', keep_default_na=False)

#  Extract only words that have an emotional association (association == 1)
emotional_words = emolex_df[emolex_df['association'] == 1]['word'].unique()

#  Initialize the TF-IDF Vectorizer using only words from the emotional lexicon
vec = TfidfVectorizer(vocabulary=emotional_words, use_idf=False, norm='l1')

#  Convert the 'tekst' column into a word-document matrix
matrix = vec.fit_transform(df['tekst'].dropna())  # Drop NaN values

# Extract the matched vocabulary words
vocab = vec.get_feature_names_out()

#  Convert the matrix to a DataFrame
wordcount_df = pd.DataFrame(matrix.toarray(), columns=vocab)

# Show the first few rows
print(wordcount_df.head())

"""# Sentiment Analyse Speeches

Laten we op zoek gaan naar de positieve emoties in NRC en de speeches.  stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?
"""

positive_words = emolex_df[(emolex_df['emotion'] == 'positive') & (emolex_df['association'] == 1)]['word'].tolist()

print(positive_words[:20])

# 1. Verkrijg de lijst met positieve woorden
positive_words = emolex_df[
    (emolex_df['emotion'] == 'positive') & (emolex_df['association'] == 1)
]['word'].tolist()

# 2. Filteren de positieve woorden uit de speeches
positive_word_df = wordcount_df[positive_words]

# 3. Gebruik TDF-IDF om de positiviteits-score te wegen.
df['positive_score'] = positive_word_df.sum(axis=1)

# 4. Laat de tien positiefste speeches zien.
print(df[['tekst', 'positive_score']].sort_values(by='positive_score', ascending=False).head(10))

"""Negatieve emoties in NRC en de speeches. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

positive_words = emolex_df[(emolex_df['emotion'] == 'negative') & (emolex_df['association'] == 1)]['word'].tolist()

print(negative_words[:20])

# 1. Verkrijg de lijst met angstige woorden
negative_words_words = emolex_df[
    (emolex_df['emotion'] == 'negative') & (emolex_df['association'] == 1)
]['word'].tolist()

# 2. Filteren de anstige woorden uit de speeches
negative_word_df = wordcount_df[negative_words]

# 3. Gebruik TDF-IDF om de angst-score te wegen.
df['negative_score'] = negative_word_df.sum(axis=1)

# 4. Laat de tien angstigste speeches zien.
print(df[['tekst', 'negative_score']].sort_values(by='negative_score', ascending=False).head(10))

"""Hoe zit het met het angst-niveau in Rutte's speeches?"""

# Get all words labeled as "fear" in the NRC lexicon
fear_words = emolex_df[(emolex_df['emotion'] == 'fear') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(fear_words[:20])  # Display first 20 words

# 1. Verkrijg de lijst met angstige woorden
fear_words = emolex_df[
    (emolex_df['emotion'] == 'fear') & (emolex_df['association'] == 1)
]['word'].tolist()

# 2. Filteren de anstige woorden uit de speeches
fear_word_df = wordcount_df[fear_words]

# 3. Gebruik TDF-IDF om de angst-score te wegen.
df['fear_score'] = fear_word_df.sum(axis=1)

# 4. Laat de tien angstigste speeches zien.
print(df[['tekst', 'fear_score']].sort_values(by='fear_score', ascending=False).head(10))

"""En dan de verwachtingen in de speeches (anticipation) en in NRC. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

# Get all words labeled as "fear" in the NRC lexicon
anticipation_words = emolex_df[(emolex_df['emotion'] == 'anticipation') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(fear_words[:20])  # Display first 20 words

# ‚úÖ 1. Extract fear words from the NRC Lexicon
anticipation_words = emolex_df[(emolex_df['emotion'] == 'fear') & (emolex_df['association'] == 1)]['word'].tolist()

# ‚úÖ 2. Apply the function to count fear words in each speech
df['anticipation_score'] = count_emotion_words(wordcount_df, anticipation_words)

# ‚úÖ 3. Display speeches with the highest fear score
print(df[['tekst', 'anticipation_score']].sort_values(by='anticipation_score', ascending=False).head(10))

"""Afkeer (disgust) in de speeches. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

# Get all words labeled as "fear" in the NRC lexicon
disgust_words = emolex_df[(emolex_df['emotion'] == 'disgust') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(disgust_words[:20])  # Display first 20 words

# ‚úÖ 1. Extract fear words from the NRC Lexicon
disgust_words = emolex_df[(emolex_df['emotion'] == 'disgust') & (emolex_df['association'] == 1)]['word'].tolist()

# ‚úÖ 2. Apply the function to count fear words in each speech
df['disgust_score'] = count_emotion_words(wordcount_df, disgust_words)

# ‚úÖ 3. Display speeches with the highest fear score
print(df[['tekst', 'disgust_score']].sort_values(by='disgust_score', ascending=False).head(10))

"""Verdriet (sadness) in speeches en NRC. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

# Get all words labeled as "fear" in the NRC lexicon
sadness_words = emolex_df[(emolex_df['emotion'] == 'sadness') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(sadness_words[:20])  # Display first 20 words

# ‚úÖ 1. Extract fear words from the NRC Lexicon
sadness_words = emolex_df[(emolex_df['emotion'] == 'sadness') & (emolex_df['association'] == 1)]['word'].tolist()

# ‚úÖ 2. Apply the function to count fear words in each speech
df['sadness_score'] = count_emotion_words(wordcount_df, sadness_words)

# ‚úÖ 3. Display speeches with the highest fear score
print(df[['tekst', 'sadness_score']].sort_values(by='sadness_score', ascending=False).head(10))

"""Verrassing (surprise) in Speeches en NRC. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

# Get all words labeled as "fear" in the NRC lexicon
surprise_words = emolex_df[(emolex_df['emotion'] == 'surprise') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(surprise_words[:20])  # Display first 20 words

# ‚úÖ 1. Extract fear words from the NRC Lexicon
sadness_words = emolex_df[(emolex_df['emotion'] == 'surprise') & (emolex_df['association'] == 1)]['word'].tolist()

# ‚úÖ 2. Apply the function to count fear words in each speech
df['surprise_score'] = count_emotion_words(wordcount_df, surprise_words)

# ‚úÖ 3. Display speeches with the highest fear score
print(df[['tekst', 'surprise_score']].sort_values(by='surprise_score', ascending=False).head(10))

"""Vertrouwen (trust) in NRC en Speeches. Eerste stap: welke woorden worden er gelabeld onder deze categorie in NRC Lexicon? Daarna: welke speeches kennen de hoogste score?"""

# Get all words labeled as "fear" in the NRC lexicon
trust_words = emolex_df[(emolex_df['emotion'] == 'trust') & (emolex_df['association'] == 1)]['word'].tolist()

# Show the first 20 fear words
print(trust_words[:20])  # Display first 20 words

# ‚úÖ 1. Extract trust words from the NRC Lexicon
trust_words = emolex_df[(emolex_df['emotion'] == 'trust') & (emolex_df['association'] == 1)]['word'].tolist()

# ‚úÖ 2. Apply the function to count trust words in each speech
df['trust_score'] = count_emotion_words(wordcount_df, trust_words)

# ‚úÖ 3. Display speeches with the highest trust score
print(df[['tekst', 'trust_score']].sort_values(by='trust_score', ascending=False).head(10))

"""En wat is dan de verhouding tussen het uitstralen van vertrouwen ('Trust') en het wijzen op angst? Hieronder worden speeches afgezet op de Vertrouwen-en-angst-balans. De hele speech bekijken? Doe dit dat in het dataset. Je krijgt hier een eerste computergestuurde analyse, maar nog geen volledig antwoord."""

#  Zorg ervoor dat de kolommen ‚Äòdate‚Äô en ‚Äòfilename‚Äô in df aanwezig zijn.
if 'date' not in df.columns:
    df['date'] = pd.NaT
if 'filename' not in df.columns:
    df['filename'] = "Unknown"

#  Maak een DataFrame waarin de scores voor vertrouwen en angst worden vergeleken met metadata.
trust_fear_df = df[['tekst', 'trust_score', 'fear_score']].copy()

# Bereken het verschil tussen vertrouwen en angst
trust_fear_df['trust_minus_fear'] = trust_fear_df['trust_score'] - trust_fear_df['fear_score']

# Toon toespraken waarin vertrouwen het hoogst is ten opzichte van angst
print("Top 10 Most Trust-Dominant Speeches:")
print(trust_fear_df.sort_values(by='trust_minus_fear', ascending=False).head(10))

#  Toon toespraken waarin angst het grootst is in vergelijking met vertrouwen
print("Top 10 Most Fear-Dominant Speeches:")
print(trust_fear_df.sort_values(by='trust_minus_fear', ascending=True).head(10))

"""Dit ziet er tof uit! Je wilt dit natuurlijk visualiseren. Kom maar door met de mathplot-bibliotheek!"""

import matplotlib.pyplot as plt

# ‚úÖ 1. Identify highest trust and highest fear speeches
high_trust = trust_fear_df.nlargest(1, 'trust_score')
high_fear = trust_fear_df.nlargest(1, 'fear_score')

# ‚úÖ 2. Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['trust_score'], df['fear_score'], alpha=0.6, color='blue', label="Speeches")
plt.scatter(high_trust['trust_score'], high_trust['fear_score'], color='green', s=100, label="Most Trusting Speech")
plt.scatter(high_fear['trust_score'], high_fear['fear_score'], color='red', s=100, label="Most Fearful Speech")

# ‚úÖ 3. Add labels & title
plt.xlabel("Trust Score")
plt.ylabel("Fear Score")
plt.title("Trust vs. Fear in Speeches")
plt.legend()
plt.show()

"""# Analyse op woordniveau

Okay, we hebben gespeeld met emoties op woord- en taalniveau. Maar dat is slecht een deel van het volledige plaatje. Hoeveel woorden worden er uberhaupt gebruikt in zijn speeches? Welke woorden komen het meest voor? Welke organisaties komen er vaak voor? En welke landen en personen? Hiervoor gebruiken we Named Entity Recognition, een bruikbare techniek om informatie uit tekst te halen.

Voor we daar zijn, willen we de teksten extra opschonen, en stopwoorden verwijderen. Je wilt niet alle keren dat Rutte 'and' of 'or' benoemd meenemen. Laten we eerst tellen hoeveel unieke woorden er alle speeches gebruikt worden.
"""

# ‚úÖ 1. Alle speeches combineren tot √©√©n tekst
all_text = " ".join(df['tekst'].dropna()).lower()

# ‚úÖ 2. Woorden een 'token' geven (tokeniseren)
from collections import Counter
import re

words = re.findall(r'\b\w+\b', all_text)  # Alle woorden extraheren
word_counts = Counter(words)  # de keren dat een woord voorkomt tellen

# ‚úÖ 3. Toon hoeveel woorden er gebruikt worden en hoeveel unieke
print(f"Total Words: {sum(word_counts.values())}")
print(f"Unique Words: {len(word_counts)}")

"""Hieronder verwijderen we de stopwoorden, hiervoor maken we gebruik van de Natural Language Toolkit (NLTK). Meer daarvoor vind je hier: https://www.nltk.org/"""

import nltk
from nltk.corpus import stopwords
from collections import Counter
import re

# ‚úÖ 1. Download stopwoorden
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))  # Laad Engelstalige stopwoorden

# ‚úÖ 2. Combineer de speeches en tokeniseer
all_text = " ".join(df['tekst'].dropna()).lower()
words = re.findall(r'\b\w+\b', all_text)  # Extract words using regex

# ‚úÖ 3. Verwijder stopwoorden
filtered_words = [word for word in words if word not in stop_words]

# ‚úÖ 4. Tel hoevaak woorden voorkomen
word_counts = Counter(filtered_words)

# ‚úÖ 5. Toon het aantal verwijderde stopwoorden
print(f"Total Words (After Stopword Removal): {sum(word_counts.values())}")
print(f"Unique Words (After Stopword Removal): {len(word_counts)}")

"""Zo, de overbodige woorden zijn weg! Foetsie! Verdwenen! Maar... Welke woorden komen het meest voor in Rutte's openbare speeches?"""

# ‚úÖ Verkrijg de 20 meest voorkomende woorden na het verwijderen van stopwoorden.
most_common_words = word_counts.most_common(20)

# ‚úÖ Toon de resultaten
print("üîπ **De 20 meest voorkomende woorden in Rutte's speeches**")
for word, count in most_common_words:
    print(f"{word}: {count}")

"""Laten we dat visualeren!

"""

# ‚úÖ Converteer de top-20 naar een DataFrame
common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Count'])

# ‚úÖ Maak een bar-chart
plt.figure(figsize=(12, 5))
plt.bar(common_words_df['Word'], common_words_df['Count'], color='blue')

# ‚úÖ Labels & Titel
plt.xticks(rotation=45)
plt.xlabel("Woord")
plt.ylabel("Frequentie")
plt.title("De 20 meest voorkomende woorden in Rutte's speeches")

# ‚úÖ Show the plot
plt.show()

"""Welke persoonsnamen worden het vaakst genoemd? De Natural Language Processing toepassing spaCy biedt uitstekende ondersteuning voor NER, en maakt het mogelijk om entiteiten uit elk soort tekst te extraheren.

Voordat we spaCy in een notebook gebruiken, moeten we een speciaal taalmodel inladen. We gaan en_core_web_sm gebruiken, omdat dit model klein en snel te importeren is. Dit is een Engelstalig model (het 'en'-gedeelte) van de naam van het model. Er zijn ook Nederlandstalige spaCymodellen. Erg handig voor de analyse van Nederlandstalige teksten.
"""

import spacy
from collections import Counter

# ‚úÖ Laad het Engelse model
nlp = spacy.load("en_core_web_sm")

"""Hieronder laten we spaCy door de speeches heengaan om persoonsnamen te identificeren. Dit zijn wel namen die ook in het taalmodel voorkomen, dus het is mogelijk niet toerijkend voor het identificeren van alle namen."""

def extract_person_names(texts):
    """
      Haal namen van personen uit een lijst met teksten en tel ze met behulp van spaCy's Named Entity Recognition (NER).

    Parameters:
        texts (list): List of speech texts.

    Returns:
        Counter: A dictionary-like object with person names as keys and their counts as values.
    """
    person_counter = Counter()

    for doc in nlp.pipe(texts, disable=["tagger", "parser"]):  # Faster processing using pipes
        for ent in doc.ents:
            if ent.label_ == "PERSON":  # Only count named persons
                person_counter[ent.text] += 1

    return person_counter

# Apply to the 'tekst' column
person_counts = extract_person_names(df['tekst'].dropna().tolist())

# Toon de 10 meest genoemde personen
print("**De 10 meest genoemde personen in Rutte's speeches**")
for name, count in person_counts.most_common(10):
    print(f"{name}: {count}")

"""Ook dat kunnen we (simpel) visualiseren!"""

# Converteer de top-10 naar een DataFrame
top_persons = pd.DataFrame(person_counts.most_common(10), columns=["Person", "Count"])

# Maak bar chart
plt.figure(figsize=(12, 5))
plt.bar(top_persons["Person"], top_persons["Count"], color="purple")

# Labels & Titel
plt.xlabel("Personen")
plt.ylabel("Vermeldingen")
plt.title("De 10 meest genoemde personen in Rutte's speeches")
plt.xticks(rotation=45)

# ‚úÖ Show the plot
plt.show()

"""Je snapt het trucje nu vast wel. Dit kunnen we ook met organisatienamen! Als je de lijst met veel voorkomende woorden hebt gezien, is het vast geen verrassing welke organisatie bovenaankomt."""

def extract_organization_names(texts):
    """
    Haal namen van organisaties uit een lijst met teksten en tel ze met behulp van spaCy's Named Entity Recognition (NER).

    Parameters:
        texts (list): List of speech texts.

    Returns:
        Counter: A dictionary-like object with organization names as keys and their counts as values.
    """
    org_counter = Counter()

    for doc in nlp.pipe(texts, disable=["tagger", "parser"]):  # Faster processing using pipes
        for ent in doc.ents:
            if ent.label_ == "ORG":  # Only count organizations
                org_counter[ent.text] += 1

    return org_counter

#  Apply to the 'tekst' column
org_counts = extract_organization_names(df['tekst'].dropna().tolist())

# Show the 10 most mentioned organizations
print(" **Top 10 Meest genoemde organisaties**")
for org, count in org_counts.most_common(10):
    print(f"{org}: {count}")

"""Okay! We kunnen ook laten analyseren welke speech de meeste organisaties noemt."""

def count_organizations_per_speech(df):
    """
    Telt het aantal unieke organisaties dat in elke toespraak wordt genoemd.

    Parameters:
        df (pd.DataFrame): Speech dataset with a 'tekst' column.

    Returns:
        pd.DataFrame: A DataFrame with the number of unique organizations per speech.
    """
    org_counts = []

    for text in df['tekst'].dropna():  # Ensure proper indexing
        doc = nlp(text)
        org_mentions = {ent.text for ent in doc.ents if ent.label_ == "ORG"}  # Use a set to avoid duplicates
        org_counts.append(len(org_mentions))

    # Ensure the list length matches the DataFrame length
    df = df.loc[df['tekst'].notna()]  # Drop rows with missing text
    df['org_mention_count'] = org_counts  # Assign values correctly

    return df

#  Apply function to count unique organization mentions per speech
df = count_organizations_per_speech(df)

#  Show the top 10 speeches with the highest number of organizations mentioned
top_speeches = df[['tekst', 'org_mention_count']].sort_values(by='org_mention_count', ascending=False).head(10)

print("üèõ **Top 10 Speeches met de meeste organisaties**")
print(top_speeches)

"""Als we toch bezig zijn... Landen en regio's:"""

def extract_geopolitical_entities(texts):
    """
    Extracteert en telt geopolitieke entiteiten (landen, steden, regio's) uit een lijst met teksten met behulp van spaCy's NER.

    Parameters:
        texts (list): List of speech texts.

    Returns:
        Counter: A dictionary-like object with country/city names as keys and their counts as values.
    """
    gpe_counter = Counter()

    for doc in nlp.pipe(texts, disable=["tagger", "parser"]):  # Use spaCy's pipeline for efficiency
        for ent in doc.ents:
            if ent.label_ == "GPE":  # Only count geopolitical entities (countries, cities, regions)
                gpe_counter[ent.text] += 1

    return gpe_counter

#  Apply function to the 'tekst' column
gpe_counts = extract_geopolitical_entities(df['tekst'].dropna().tolist())

#  Show the 10 most mentioned geopolitical entities
print("üåç **Top 10 meest genoemde landen/regio's**")
for gpe, count in gpe_counts.most_common(10):
    print(f"{gpe}: {count}")

"""En visualiseren! Dan valt je meteen op dat Isra√´l niet voorkomt in de eerste uitingen van Rutte."""

#  Convert to a DataFrame for easy plotting
top_gpes = pd.DataFrame(gpe_counts.most_common(10), columns=["Country/Region", "Count"])

#  Create bar chart
plt.figure(figsize=(12, 5))
plt.bar(top_gpes["Country/Region"], top_gpes["Count"], color="darkred")

#  Labels & Title
plt.xlabel("Land/Regio")
plt.ylabel("Vermeldingen")
plt.title("Top 10 meest genoemde landen/regio's in speeches Rutte")
plt.xticks(rotation=45)

# ‚úÖShow the plot
plt.show()

"""Dit is slechts een eerste basis voor het daadwerkelijk onderzoeken van teksten, maar dit notebook laat zien wat je kan met taalverwerking en computationele methoden kunt doen. Dat gezegd hebbende: er kleven ook nadelen aan. Alle NER-systemen hebben zwakke punten en zijn vatbaar voor fouten.

Mijn tip: beschouw de emotieherkenning en NER als een van de stappen in het researchproces, en eventueel als een zoekmachine die je een lijst aan resultaten biedt, maar die niet 100% kloppend is.  Het is altijd een goed idee om steekproefsgewijs resultaten te controleren en te beoordelen.

Je zult alsnog de documenten moeten lezen (met een gezonde dosis scepsis), maar het kan je zeker helpen bij onderzoek en analyse.

Meer weten? Tips? Vragen? Contacteer mij op j.decooker@fontys.nl of ons lectoraat [Ontwerpen aan de Journalistiek](https://ontwerpenaandejournalistiek.nl/) van Fontys Journalistiek.
"""